{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 561) (7352,) (2947, 561) (2947,) (561,)\n",
      "(7209, 561) (7209,) (3090, 561) (3090,) (561,)\n",
      "Training data shape:  (7209, 561)\n",
      "Testing data shape:  (3090, 561)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from langchain_groq.chat_models import ChatGroq\n",
    "import tsfel\n",
    "\n",
    "# Available Models\n",
    "groq_models = {\n",
    "    \"llama3-70b\": \"llama3-70b-8192\",\n",
    "    \"gemma-7b\": \"gemma-7b-it\",\n",
    "    \"llama3.1-70b\":\"llama-3.1-70b-versatile\",\n",
    "    \"llama3-8b\":\"llama3-8b-8192\"\n",
    "    }\n",
    "model = groq_models[\"llama3-70b\"]\n",
    "\n",
    "# Constants\n",
    "GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\n",
    "green = \"\\x1b[32;40m\"\n",
    "red =   \"\\x1b[31;40m\"\n",
    "reset = \"\\x1b[0m\"       # color reset\n",
    "classes = {\"WALKING\":1,\"WALKING_UPSTAIRS\":2,\"WALKING_DOWNSTAIRS\":3,\"SITTING\":4,\"STANDING\":5,\"LAYING\":6}\n",
    "folders = list(classes.keys())\n",
    "N = 30      # There are too many samples, so we will take 1 out of N\n",
    "\n",
    "X_train = np.loadtxt(\"./HAR/UCI HAR Dataset/train/X_train.txt\")\n",
    "y_train = np.loadtxt(\"./HAR/UCI HAR Dataset/train/y_train.txt\",dtype=np.int32)\n",
    "X_test = np.loadtxt(\"./HAR/UCI HAR Dataset/test/X_test.txt\")\n",
    "y_test = np.loadtxt(\"./HAR/UCI HAR Dataset/test/y_test.txt\",dtype=np.int32)\n",
    "features = pd.read_csv(\"./HAR/UCI HAR Dataset/features.txt\",sep=\"\\s+\",header=None)[1]\n",
    "\n",
    "print(X_train.shape,y_train.shape,X_test.shape,y_test.shape,features.shape)\n",
    "\n",
    "X = np.concatenate((X_train,X_test))\n",
    "y = np.concatenate((y_train,y_test))\n",
    "\n",
    "# split the data into training and testing sets. Change the seed value to obtain different random splits.\n",
    "seed = 100\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=seed,stratify=y)\n",
    "\n",
    "X_train = pd.DataFrame(X_train,columns=features)\n",
    "X_test = pd.DataFrame(X_test,columns=features)\n",
    "\n",
    "print(X_train.shape,y_train.shape,X_test.shape,y_test.shape,features.shape)\n",
    "\n",
    "print(\"Training data shape: \",X_train.shape)\n",
    "print(\"Testing data shape: \",X_test.shape)\n",
    "\n",
    "X_examples = []\n",
    "y_examples = []\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    if y_train[i] not in y_examples:\n",
    "        X_examples.append(X_train.iloc[i,:])\n",
    "        y_examples.append(y_train[i])\n",
    "\n",
    "zero_shot_prompt = lambda data:f\"\"\"\n",
    "* Your task is to classify the given featurised 3-axis accelerometer data into one of the following activity labels:\n",
    "1) WALKING\n",
    "2) SITTING\n",
    "3) STANDING\n",
    "4) WALKING_UPSTAIRS\n",
    "5) WALKING_DOWNSTAIRS\n",
    "6) LAYING\n",
    "* Only output the identified label and nothing else.\n",
    "* Do not provide any explanation or analysis.\n",
    "Acceleration Data:\n",
    "{data}\n",
    "\"\"\"\n",
    "\n",
    "examples = \"\\n\".join([f\"EXAMPLE {i} DATA :\\n{X_examples[i]}\\nEXAMPLE {i} LABEL : {folders[y_examples[i]-1]}\" for i in range(len(X_examples))])\n",
    "\n",
    "few_shot_prompt = lambda data:f\"\"\"\n",
    "* You are HAR tool.\n",
    "* Your task is to analyze the provided labeled featurised 3-axis accelerometer data and learn the patterns associated with the label in order to identify unlabeled data.\n",
    "* Only give output in one word and do not provide any explanation.\n",
    "{examples}\n",
    "\n",
    "TEST DATA: {data}\n",
    "LABEL for TEST DATA: ?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama3-70b-8192: Test case #103 Output: \u001b[31;40mSTANDING           \u001b[0mActual: WALKING              Correct: 24\n",
      "\n",
      "Model:              llama3-70b-8192\n",
      "Total Test Cases:   103\n",
      "Correct Predictions:24\n",
      "Accuracy:           0.2330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zero_shot_correct_count = 0\n",
    "\n",
    "llm = ChatGroq(model=model, api_key=GROQ_API_KEY, temperature=0)\n",
    "zero_shot_ans = []\n",
    "\n",
    "for i in range(len(X_test)//N):\n",
    "\n",
    "    query = zero_shot_prompt(X_test.iloc[i,:])\n",
    "    ans = llm.invoke(query).content\n",
    "    zero_shot_ans.append(ans)\n",
    "    if(ans==folders[y_test[i]-1]):\n",
    "        zero_shot_correct_count+=1\n",
    "        color = green\n",
    "    else:\n",
    "        color = red\n",
    "\n",
    "    print(f\"\\r{model:<15}: Test case #{i+1:<3} Output: {color}{ans:<18} {reset}Actual: {folders[y_test[i]-1]:<20} Correct: {zero_shot_correct_count}\",end=\"\")\n",
    "\n",
    "print()\n",
    "print(f\"\"\"\n",
    "Model:              {model}\n",
    "Total Test Cases:   {len(X_test)//N}\n",
    "Correct Predictions:{zero_shot_correct_count}\n",
    "Accuracy:           {zero_shot_correct_count/(len(X_test)//N):.4f}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama3-70b-8192: Test case #102 Output: \u001b[31;40mSTANDING           \u001b[0mActual: WALKING              Correct:28\n",
      "\n",
      "Model:               llama3-70b-8192\n",
      "Total Test Cases:    103\n",
      "Correct Predictions: 28\n",
      "Accuracy:            0.27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "few_shot_correct_count = 0\n",
    "\n",
    "llm = ChatGroq(model=model, api_key=GROQ_API_KEY, temperature=0)\n",
    "few_shot_ans = []\n",
    "\n",
    "for i in range(len(X_test)//N):\n",
    "\n",
    "    query = few_shot_prompt(X_test.loc[i,:])\n",
    "    ans = llm.invoke(query).content\n",
    "    few_shot_ans.append(ans)\n",
    "    \n",
    "    if(ans==folders[y_test[i]-1]):\n",
    "        few_shot_correct_count+=1\n",
    "        color = green\n",
    "    else:\n",
    "        color = red\n",
    "\n",
    "    print(f\"\\r{model:<15}: Test case #{i:<3} Output: {color}{ans:<18} {reset}Actual: {folders[y_test[i]-1]:<20} Correct:{few_shot_correct_count}\",end=\"\")\n",
    "\n",
    "print()\n",
    "print(f\"\"\"\n",
    "Model:               {model}\n",
    "Total Test Cases:    {len(X_test)//N}\n",
    "Correct Predictions: {few_shot_correct_count}\n",
    "Accuracy:            {few_shot_correct_count/(len(X_test)//N):.2f}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between Few-shot and Zero-shot\n",
    "\n",
    "Although the accuracy depends on the choices of examples and test cases provided, Few-shot generally gives more accuracy compared to Zero-shot, unless the examples provided are very biased.\n",
    "\n",
    "The reason of this difference between the accuracies lie between the fact that we provide some examples of all the possible  classification allowing the LLM to refer to examples and compare the test data, whereas in Zero-shot this was not possible, as LLMs are not trained on large numerical data for HAR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Few-shot and Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=46, min_samples_split=4, max_features='sqrt')\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:<.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree performs better than the Few-shot because few shots are not enough for the LLM to find the complex patterns in the data, and thus making it more erroreous.\n",
    "\n",
    "In Decision Tree, the model is trained over the data iteratively in order to find intricate patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of Zero-shot and Few-Shot in HAR\n",
    "\n",
    "In Zero-shot learning as the LLM has no data to learn from, it is more prone to errors, but is comparatively faster than the Few-shot, as Few-shot has to first learn from the examples provided and then analyze the input.\n",
    "\n",
    "In Few-shot learning, the LLM has to be provided correct examples which are sometimes unknown or are biased. This biasness in the examples can make the Few-shot learning more biased, giving less accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with New Activity\n",
    "\n",
    "We have taken \"JOGGING\" as new activity. We downloaded the data from https://www.cis.fordham.edu/wisdm/dataset.php as raw csv.\n",
    "Then we took sample data from user id 33 and pre-processed the raw data to remove all the other activities and user ids.\n",
    "\n",
    "The sample data file is saved as 'HAR/jogging.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = 10\n",
    "offset = 100\n",
    "\n",
    "df = pd.read_csv(\"./HAR/jogging.csv\",sep=\",\",header=0,lineterminator=';')\n",
    "\n",
    "cgf = tsfel.get_features_by_domain(\"statistical\")  # All statistical domain features will be extracted\n",
    "cgf = tsfel.get_features_by_domain(\"temporal\")     # All temporal domain features will be extracted\n",
    "cgf = tsfel.get_features_by_domain(\"spectral\")     # All spectral domain features will be extracted\n",
    "\n",
    "df = df.iloc[offset:offset+(time*50),1:]\n",
    "\n",
    "X_new_test = tsfel.time_series_features_extractor(cgf,df,fs=50)\n",
    "\n",
    "query = zero_shot_prompt(X_new_test)\n",
    "\n",
    "ans = llm.invoke(query).content\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that as the LLM was not provided examples associated with the JOGGING activity, it is unable to identify it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Few Shot with Random Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_random = pd.DataFrame(np.random.random(X_test.shape),columns=features)\n",
    "y_random = np.random.randint(1,len(folders),len(X_test))\n",
    "\n",
    "rand_correct_count = 0\n",
    "rand_ans = []\n",
    "\n",
    "llm = ChatGroq(model=model, api_key=GROQ_API_KEY, temperature=0)\n",
    "\n",
    "for i in range(len(X_random)//N):\n",
    "\n",
    "    query = zero_shot_prompt(X_random.iloc[i])\n",
    "    ans = llm.invoke(query).content\n",
    "    rand_ans.append(ans)\n",
    "\n",
    "    if(ans==folders[y_random[i]-1]):\n",
    "        rand_correct_count+=1\n",
    "        color = green\n",
    "    else:\n",
    "        color = red\n",
    "\n",
    "    print(f\"\\r{model:<15}: Test case #{i:<3} Output: {color}{ans:<18} {reset}Actual: {folders[y_random[i]-1]:<20} Correct:{rand_correct_count}\",end=\"\")\n",
    "    \n",
    "print()\n",
    "print(f\"\"\"\n",
    "Model:               {model}\n",
    "Total Test Cases:    {len(X_random)//N}\n",
    "Correct Predictions: {rand_correct_count}\n",
    "Accuracy:            {rand_correct_count/(len(X_random)//N):.2f}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
